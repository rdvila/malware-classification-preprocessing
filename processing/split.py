#!/usr/bin/env python

import argparse
import glob
import os
import sys
import dask.dataframe as ddf
from cloudpickle import dumps
from random import randint

from dask_ml.preprocessing import MinMaxScaler

prefix = "/opt/ml"

labels = ['adware', 'flooder', 'ransomware', 'dropper', 'spyware', 'packed',
          'crypto_miner', 'file_infector', 'installer', 'worm', 'downloader']
features = [f"feature_{x}" for x in range(2381)]


def log_container_info(prefix):
    print('------------------- environment variables -------------------')
    print(os.environ)
    print('------------------- environment variables -------------------')
    print('------------------- arguments -------------------')
    print(sys.argv)
    print('------------------- arguments -------------------')
    print('------------------- filesystem -------------------')
    for filename in glob.iglob(prefix + '**/**', recursive=True):
        print(filename)
    print('------------------- filesystem -------------------')


def process(args):
    print('Starting the processing.')

    df = ddf.read_parquet(
        f"{prefix}/processing/data/", compression="snappy", columns=(features+labels))

    print(df.dtypes)

    print("create types dict")
    dict_label_types = dict()
    for label in labels:
    	dict_label_types[label] = args.label_type
    dict_feature_types = dict()
    for feature in features:
    	dict_feature_types[feature] = args.feature_type
    
    print("split dataframe")
    train, test, val = df.random_split([0.7, 0.2, 0.1], random_state=args.random_state, shuffle=True)

    print("scale x train/test/val")
    scaler = MinMaxScaler()
    scaler.fit(train[features])

    print("scale transform")
    scaled_train, scaled_test, scaled_val = scaler.transform(train[features]), scaler.transform(test[features]), scaler.transform(val[features])

    print("save x train, test and val")
    scaled_train.astype(dict_feature_types).to_parquet(
        f"{prefix}/processing/output/{args.train_name}/x/", compression="snappy", compute=True)
    scaled_test.astype(dict_feature_types).to_parquet(
        f"{prefix}/processing/output/{args.test_name}/x/", compression="snappy", compute=True)
    scaled_val.astype(dict_feature_types).to_parquet(
        f"{prefix}/processing/output/{args.val_name}/x/", compression="snappy", compute=True)


    print("save y train, test and val")
    train[labels].astype(dict_label_types).to_parquet(
        f"{prefix}/processing/output/{args.train_name}/y/", compression="snappy", compute=True)
    test[labels].astype(dict_label_types).to_parquet(
        f"{prefix}/processing/output/{args.test_name}/y/", compression="snappy", compute=True)
    val[labels].astype(dict_label_types).to_parquet(
        f"{prefix}/processing/output/{args.val_name}/y/", compression="snappy", compute=True)


    print("save scaler")
    filename = f"{prefix}/processing/output/support/scaler.pkl"
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    with open(filename, "wb") as f:
        f.write(dumps(scaler))

    print('Finish the processing.')


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument("-train-name", type=str, default="train")
    parser.add_argument("-test-name", type=str, default="test")
    parser.add_argument("-val-name", type=str, default="val")
    parser.add_argument("-label-type", type=str, default="int8")
    parser.add_argument("-feature-type", type=str, default="float32")
    parser.add_argument("-random-state", type=int, default=randint(0, 2**32))
    args = parser.parse_args()

    log_container_info(prefix)
    process(args)

    sys.exit(0)
