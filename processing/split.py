#!/usr/bin/env python

import argparse
import glob
import os
import sys
import dask.dataframe as ddf
from cloudpickle import dumps
from random import randint
from dask_ml.model_selection import train_test_split
from dask_ml.preprocessing import MinMaxScaler

labels = ['adware', 'flooder', 'ransomware', 'dropper', 'spyware', 'packed',
          'crypto_miner', 'file_infector', 'installer', 'worm', 'downloader']
features = [f"feature_{x}" for x in range(2381)]


def log_container_info(prefix):
    print('------------------- environment variables -------------------')
    print(os.environ)
    print('------------------- environment variables -------------------')
    print('------------------- arguments -------------------')
    print(sys.argv)
    print('------------------- arguments -------------------')
    print('------------------- filesystem -------------------')
    for filename in glob.iglob(prefix + '**/**', recursive=True):
        print(filename)
    print('------------------- filesystem -------------------')


def process(prefix, test_size, x_train_name, y_train_name,
            x_test_name, y_test_name, random_state):
    print('Starting the processing.')

    df_X = ddf.read_parquet(
        f"{prefix}/processing/data/", compression="snappy", columns=features)
    df_y = ddf.read_parquet(
        f"{prefix}/processing/data/", compression="snappy", columns=labels)

    print("split train/test")
    X_train, X_test, y_train, y_test = train_test_split(
        df_X, df_y, test_size=test_size, random_state=random_state, shuffle=True)

    print("scale x train/test")
    scaler = MinMaxScaler()
    scaler.fit(X_train)
    X_train_scaled = scaler.transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    print("save X train and test")
    X_train_scaled.to_parquet(
        f"{prefix}/processing/output/{x_train_name}/", compression="snappy", compute=True)
    X_test_scaled.to_parquet(
        f"{prefix}/processing/output/{x_test_name}/", compression="snappy", compute=True)

    print("save y train and test")
    y_train.to_parquet(
        f"{prefix}/processing/output/{y_train_name}/", compression="snappy", compute=True)
    y_test.to_parquet(
        f"{prefix}/processing/output/{y_test_name}/", compression="snappy", compute=True)

    print("save scaler")
    with open(f"{prefix}/processing/output/scaler.pkl", "wb") as f:
        f.write(dumps(scaler))

    print('Finish the processing.')


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument("-prefix", type=str, default="/opt/ml")
    parser.add_argument("-test-size", type=float, default=0.2)
    parser.add_argument("-x-train-name", type=str, default="x-train")
    parser.add_argument("-y-train-name", type=str, default="y-train")
    parser.add_argument("-x-test-name", type=str, default="x-test")
    parser.add_argument("-y-test-name", type=str, default="y-test")
    parser.add_argument("-random-state", type=int, default=randint(0, 2**32))
    args = parser.parse_args()

    log_container_info(args.prefix)
    process(prefix=args.prefix, test_size=args.test_size, x_train_name=args.x_train_name, y_train_name=args.y_train_name,
            x_test_name=args.x_test_name, y_test_name=args.y_test_name, random_state=args.random_state)

    sys.exit(0)
