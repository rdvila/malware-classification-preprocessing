#!/usr/bin/env python

import argparse
import glob
import os
import sys
import dask.dataframe as ddf
from cloudpickle import dumps
from random import randint
from dask_ml.model_selection import train_test_split
from dask_ml.preprocessing import MinMaxScaler

prefix = "/opt/ml"

labels = ['adware', 'flooder', 'ransomware', 'dropper', 'spyware', 'packed',
          'crypto_miner', 'file_infector', 'installer', 'worm', 'downloader']
features = [f"feature_{x}" for x in range(2381)]


def log_container_info(prefix):
    print('------------------- environment variables -------------------')
    print(os.environ)
    print('------------------- environment variables -------------------')
    print('------------------- arguments -------------------')
    print(sys.argv)
    print('------------------- arguments -------------------')
    print('------------------- filesystem -------------------')
    for filename in glob.iglob(prefix + '**/**', recursive=True):
        print(filename)
    print('------------------- filesystem -------------------')


def process(args):
    print('Starting the processing.')

    df_X = ddf.read_parquet(
        f"{prefix}/processing/data/", compression="snappy", columns=features)
    df_y = ddf.read_parquet(
        f"{prefix}/processing/data/", compression="snappy", columns=labels)

    print("split train/test")
    X_train, X_split, y_train, y_split = train_test_split(
        df_X, df_y, test_size=0.4, random_state=args.random_state, shuffle=True)

    print("split test/validation")
    X_test, X_val, y_test, y_val = train_test_split(
        X_split, y_split, test_size=0.1, random_state=args.random_state, shuffle=True)

    print("scale x train/test/val")
    scaler = MinMaxScaler()
    scaler.fit(X_train)
    X_train_scaled = scaler.transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    X_val_scaled = scaler.transform(X_val)

    print("save X train, test and val")
    X_train_scaled.to_parquet(
        f"{prefix}/processing/output/{args.x_train_name}/", compression="snappy", compute=True)
    X_test_scaled.to_parquet(
        f"{prefix}/processing/output/{args.x_test_name}/", compression="snappy", compute=True)
    X_val_scaled.to_parquet(
        f"{prefix}/processing/output/{args.x_val_name}/", compression="snappy", compute=True)

    print("save y train and test")
    y_train.to_parquet(
        f"{prefix}/processing/output/{args.y_train_name}/", compression="snappy", compute=True)
    y_test.to_parquet(
        f"{prefix}/processing/output/{args.y_test_name}/", compression="snappy", compute=True)
    y_val.to_parquet(
        f"{prefix}/processing/output/{args.y_val_name}/", compression="snappy", compute=True)

    print("save scaler")
    with open(f"{prefix}/processing/output/scaler.pkl", "wb") as f:
        f.write(dumps(scaler))

    print('Finish the processing.')


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument("-x-train-name", type=str, default="x-train")
    parser.add_argument("-y-train-name", type=str, default="y-train")
    parser.add_argument("-x-test-name", type=str, default="x-test")
    parser.add_argument("-y-test-name", type=str, default="y-test")
    parser.add_argument("-x-val-name", type=str, default="x-val")
    parser.add_argument("-y-val-name", type=str, default="y-val")
    parser.add_argument("-random-state", type=int, default=randint(0, 2**32))
    args = parser.parse_args()

    log_container_info(prefix)
    process(args)

    sys.exit(0)
