#!/usr/bin/env python

import os
import sys
import glob
import shutil
from cloudpickle import dumps
import dask.dataframe as ddf
from dask_ml.model_selection import train_test_split
from dask_ml.preprocessing import MinMaxScaler
from sklearn.feature_selection import chi2, SelectKBest
from tqdm import tqdm

import argparse

labels = ['adware', 'flooder', 'ransomware', 'dropper', 'spyware', 'packed', 'crypto_miner', 'file_infector', 'installer', 'worm', 'downloader']
features = ["feature_{}".format(x) for x in range(2381)]

def process(prefix, k, chunk_size, random_state):
    print('Starting the processing.')
    print('------------------- environment variables -------------------')
    print(os.environ)
    print('------------------- environment variables -------------------')
    print('------------------- arguments -------------------')
    print(sys.argv)
    print('------------------- arguments -------------------')
    print('------------------- filesystem -------------------')
    for filename in glob.iglob(prefix + '**/**', recursive=True):
        print(filename)
    print('------------------- filesystem -------------------')
    print('------------------- processing phase 1-------------------')

    df_X = ddf.read_parquet(f"{prefix}/processing/data/", compression="snappy", columns=features)
    df_y = ddf.read_parquet(f"{prefix}/processing/data/", compression="snappy", columns=labels)

    print("split train/test")
    X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=random_state, shuffle=True)

    print("scale x train/test")
    scaler = MinMaxScaler()
    scaler.fit(X_train)
    X_train_scaled = scaler.transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    print("save X train and test")
    X_train_scaled.to_parquet(f"{prefix}/processing/tmp/sorel-x-train/", compression="snappy", compute=True)
    X_test_scaled.to_parquet(f"{prefix}/processing/tmp/sorel-x-test/", compression="snappy", compute=True)

    y_train.to_parquet(f"{prefix}/processing/output/sorel-y-train/", compression="snappy", compute=True)
    y_test.to_parquet(f"{prefix}/processing/output/sorel-y-test/", compression="snappy", compute=True)

    print("save scaler")
    with open(f"{prefix}/processing/output/scaler.pkl", "wb") as f: f.write(dumps(scaler))

    # we don't need anymore, we will read from the disk in order
    del X_train_scaled
    del X_test_scaled
    del X_train
    del X_test
    del y_train
    del y_test
    del df_X
    del df_y

    print('------------------- processing phase 1-------------------')
    print('------------------- processing phase 2-------------------')

    y = ddf.read_parquet(f"{prefix}/processing/output/sorel-y-train/", compression="snappy")

    chunked_features = []
    for i in range(0, len(features), chunk_size):
        chunked_features.append(features[i:i+chunk_size])

    selected_features = set()
    feature_scores = dict()
    for label in labels:
        print(f"kbest for {label}")
        scores = []
        for chunck in tqdm(chunked_features):
            X = ddf.read_parquet(f"{prefix}/processing/tmp/sorel-x-train/", compression="snappy", columns=chunck)
            selector = SelectKBest(chi2, k='all')
            selector.fit(X, y[label])
            scores += zip(chunck, selector.scores_)
        with open(f"{prefix}/processing/output/scores-{label}.pkl", "wb") as f: f.write(dumps(scores))
        top_k = list(sorted(scores, key=lambda x: x[1], reverse=True))[:k]
        selected_features.update([x[0] for x in top_k])
        print(top_k)

    print(f"nubmer of features selected using k best = {len(selected_features)}")
    print(f"number of features to drop using k best = {len(features) - len(selected_features)}")

    print("save selected features")
    with open(f"{prefix}/processing/output/selected_features.pkl", "wb") as f: f.write(dumps(selected_features))

    del y

    print('------------------- processing phase 2-------------------')
    print('------------------- processing phase 3-------------------')

    X_train_selected = ddf.read_parquet(f"{prefix}/processing/tmp/sorel-x-train/", compression="snappy", columns=selected_features)
    X_test_selected = ddf.read_parquet(f"{prefix}/processing/tmp/sorel-x-test/", compression="snappy", columns=selected_features)

    print("save X train and test")
    X_train_selected.to_parquet(f"{prefix}/processing/output/sorel-x-train/", compression="snappy", compute=True)
    X_test_selected.to_parquet(f"{prefix}/processing/output/sorel-x-test/", compression="snappy", compute=True)

    print('------------------- processing phase 3-------------------')

    print('Finish the processing.')


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument("-prefix", type=str, default="/opt/ml")
    parser.add_argument("-k", type=int, default=100)
    parser.add_argument("-chunk-size", type=int, default=48)
    parser.add_argument("-random-state", type=int, default=42)
    args = parser.parse_args()

    process(prefix=args.prefix, k=args.k, chunk_size=args.chunk_size, random_state=args.random_state)

    sys.exit(0)
